{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc29e0a8",
   "metadata": {},
   "source": [
    "# Building a RAG System from Scratch - Step by Step\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances Large Language Models by providing them with relevant context from a knowledge base before generating answers.\n",
    "\n",
    "**Why RAG?**\n",
    "- ‚úÖ Reduces hallucinations by grounding answers in real data\n",
    "- ‚úÖ Enables LLMs to access up-to-date information\n",
    "- ‚úÖ Allows working with private/proprietary documents\n",
    "- ‚úÖ Can cite sources for answers\n",
    "\n",
    "**What we'll build:**\n",
    "A complete RAG system that can answer questions about a document (2024 State of the Union address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac03028",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",              # Core LangChain framework\n",
    "    \"langchain-chroma\",       # Chroma vector store integration\n",
    "    \"langchain-openai\",       # OpenAI models integration\n",
    "    \"langchain-core\",         # Core LangChain utilities\n",
    "    \"python-dotenv\",          # Environment variable management\n",
    "    \"chromadb\"                # Vector database\n",
    "]\n",
    "\n",
    "print(\"Installing RAG dependencies...\\n\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(f\"‚úì {package}\")\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f0963",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "We install 6 essential packages:\n",
    "- **langchain**: Main framework for building LLM applications\n",
    "- **langchain-chroma**: Allows us to use ChromaDB as our vector database\n",
    "- **langchain-openai**: Provides OpenAI's GPT models and embeddings\n",
    "- **langchain-core**: Core utilities for chains and prompts\n",
    "- **python-dotenv**: Loads API keys from .env file securely\n",
    "- **chromadb**: Lightweight vector database for storing document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1a899",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful and environment loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfd6bb",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "Each import serves a specific purpose in our RAG pipeline:\n",
    "- **Chroma**: Vector database for storing embeddings\n",
    "- **PromptTemplate**: Structures prompts with variables\n",
    "- **RunnablePassthrough**: Passes data through pipeline unchanged\n",
    "- **StrOutputParser**: Extracts text from LLM response\n",
    "- **OpenAIEmbeddings**: Converts text to vector embeddings\n",
    "- **ChatOpenAI**: OpenAI's chat model (GPT)\n",
    "- **CharacterTextSplitter**: Splits large documents into chunks\n",
    "- **load_dotenv()**: Loads your OPENAI_API_KEY from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb8d16",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"‚úÖ Embeddings model initialized\")\n",
    "print(f\"   Model: text-embedding-3-large\")\n",
    "print(f\"   Dimensions: 3072 (vector size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5af8e1",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar concepts have similar vectors.\n",
    "\n",
    "**Why text-embedding-3-large?**\n",
    "- High quality: 3072-dimensional vectors\n",
    "- Captures nuanced meaning\n",
    "- Good for semantic similarity search\n",
    "\n",
    "**Example:** \n",
    "- \"dog\" and \"puppy\" ‚Üí similar vectors\n",
    "- \"dog\" and \"car\" ‚Üí different vectors\n",
    "\n",
    "These embeddings allow us to find relevant documents even when they don't contain exact keyword matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee9c17",
   "metadata": {},
   "source": [
    "## Step 4: Create Vector Store (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"state_of_union_rag\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store created\")\n",
    "print(f\"   Database: ChromaDB\")\n",
    "print(f\"   Collection: state_of_union_rag\")\n",
    "print(f\"   Ready to store document embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd4b9f",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "ChromaDB is a vector database that stores and retrieves embeddings efficiently.\n",
    "\n",
    "**What it does:**\n",
    "- Stores document embeddings (vectors)\n",
    "- Performs fast similarity searches\n",
    "- Returns the most relevant documents for a query\n",
    "\n",
    "**How it works:**\n",
    "1. Documents ‚Üí Embeddings ‚Üí Stored in ChromaDB\n",
    "2. Query ‚Üí Embedding ‚Üí Search similar vectors\n",
    "3. ChromaDB returns most similar documents\n",
    "\n",
    "**Why ChromaDB?**\n",
    "- Lightweight and easy to use\n",
    "- No separate server needed\n",
    "- Perfect for development and small-to-medium projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeab2e9",
   "metadata": {},
   "source": [
    "## Step 5: Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2024_state_of_the_union.txt\", \"r\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "print(\"‚úÖ Document loaded successfully\")\n",
    "print(f\"   File: 2024_state_of_the_union.txt\")\n",
    "print(f\"   Total characters: {len(document):,}\")\n",
    "print(f\"   Total words: ~{len(document.split()):,}\")\n",
    "print(f\"\\n   Preview (first 200 chars):\")\n",
    "print(f\"   {document[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fd6b3",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "We load the document that will serve as our knowledge base.\n",
    "\n",
    "**Why this step?**\n",
    "- RAG needs a source of information to retrieve from\n",
    "- This document contains facts the LLM can reference\n",
    "- In production, this could be PDFs, databases, APIs, etc.\n",
    "\n",
    "**Note:** The document is likely too large to fit in a single LLM prompt (context window), which is why we need RAG and chunking in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ed5f8",
   "metadata": {},
   "source": [
    "## Step 6: Split Document into Chunks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
