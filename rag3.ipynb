{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc29e0a8",
   "metadata": {},
   "source": [
    "# Building a RAG System from Scratch - Step by Step\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances Large Language Models by providing them with relevant context from a knowledge base before generating answers.\n",
    "\n",
    "**Why RAG?**\n",
    "- ‚úÖ Reduces hallucinations by grounding answers in real data\n",
    "- ‚úÖ Enables LLMs to access up-to-date information\n",
    "- ‚úÖ Allows working with private/proprietary documents\n",
    "- ‚úÖ Can cite sources for answers\n",
    "\n",
    "**What we'll build:**\n",
    "A complete RAG system that can answer questions about a document (2024 State of the Union address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac03028",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",              # Core LangChain framework\n",
    "    \"langchain-chroma\",       # Chroma vector store integration\n",
    "    \"langchain-openai\",       # OpenAI models integration\n",
    "    \"langchain-core\",         # Core LangChain utilities\n",
    "    \"python-dotenv\",          # Environment variable management\n",
    "    \"chromadb\"                # Vector database\n",
    "]\n",
    "\n",
    "print(\"Installing RAG dependencies...\\n\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(f\"‚úì {package}\")\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f0963",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "We install 6 essential packages:\n",
    "- **langchain**: Main framework for building LLM applications\n",
    "- **langchain-chroma**: Allows us to use ChromaDB as our vector database\n",
    "- **langchain-openai**: Provides OpenAI's GPT models and embeddings\n",
    "- **langchain-core**: Core utilities for chains and prompts\n",
    "- **python-dotenv**: Loads API keys from .env file securely\n",
    "- **chromadb**: Lightweight vector database for storing document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1a899",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful and environment loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfd6bb",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "Each import serves a specific purpose in our RAG pipeline:\n",
    "- **Chroma**: Vector database for storing embeddings\n",
    "- **PromptTemplate**: Structures prompts with variables\n",
    "- **RunnablePassthrough**: Passes data through pipeline unchanged\n",
    "- **StrOutputParser**: Extracts text from LLM response\n",
    "- **OpenAIEmbeddings**: Converts text to vector embeddings\n",
    "- **ChatOpenAI**: OpenAI's chat model (GPT)\n",
    "- **CharacterTextSplitter**: Splits large documents into chunks\n",
    "- **load_dotenv()**: Loads your OPENAI_API_KEY from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb8d16",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"‚úÖ Embeddings model initialized\")\n",
    "print(f\"   Model: text-embedding-3-large\")\n",
    "print(f\"   Dimensions: 3072 (vector size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5af8e1",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar concepts have similar vectors.\n",
    "\n",
    "**Why text-embedding-3-large?**\n",
    "- High quality: 3072-dimensional vectors\n",
    "- Captures nuanced meaning\n",
    "- Good for semantic similarity search\n",
    "\n",
    "**Example:** \n",
    "- \"dog\" and \"puppy\" ‚Üí similar vectors\n",
    "- \"dog\" and \"car\" ‚Üí different vectors\n",
    "\n",
    "These embeddings allow us to find relevant documents even when they don't contain exact keyword matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee9c17",
   "metadata": {},
   "source": [
    "## Step 4: Create Vector Store (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"state_of_union_rag\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store created\")\n",
    "print(f\"   Database: ChromaDB\")\n",
    "print(f\"   Collection: state_of_union_rag\")\n",
    "print(f\"   Ready to store document embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd4b9f",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "ChromaDB is a vector database that stores and retrieves embeddings efficiently.\n",
    "\n",
    "**What it does:**\n",
    "- Stores document embeddings (vectors)\n",
    "- Performs fast similarity searches\n",
    "- Returns the most relevant documents for a query\n",
    "\n",
    "**How it works:**\n",
    "1. Documents ‚Üí Embeddings ‚Üí Stored in ChromaDB\n",
    "2. Query ‚Üí Embedding ‚Üí Search similar vectors\n",
    "3. ChromaDB returns most similar documents\n",
    "\n",
    "**Why ChromaDB?**\n",
    "- Lightweight and easy to use\n",
    "- No separate server needed\n",
    "- Perfect for development and small-to-medium projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeab2e9",
   "metadata": {},
   "source": [
    "## Step 5: Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2024_state_of_the_union.txt\", \"r\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "print(\"‚úÖ Document loaded successfully\")\n",
    "print(f\"   File: 2024_state_of_the_union.txt\")\n",
    "print(f\"   Total characters: {len(document):,}\")\n",
    "print(f\"   Total words: ~{len(document.split()):,}\")\n",
    "print(f\"\\n   Preview (first 200 chars):\")\n",
    "print(f\"   {document[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fd6b3",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "We load the document that will serve as our knowledge base.\n",
    "\n",
    "**Why this step?**\n",
    "- RAG needs a source of information to retrieve from\n",
    "- This document contains facts the LLM can reference\n",
    "- In production, this could be PDFs, databases, APIs, etc.\n",
    "\n",
    "**Note:** The document is likely too large to fit in a single LLM prompt (context window), which is why we need RAG and chunking in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ed5f8",
   "metadata": {},
   "source": [
    "## Step 6: Split Document into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,        # Each chunk: ~1000 characters\n",
    "    chunk_overlap=200,      # Overlap: 200 characters between chunks\n",
    "    length_function=len,    # Use character count\n",
    "    separator=\"\\n\"          # Split on newlines when possible\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([document])\n",
    "\n",
    "print(\"‚úÖ Document split into chunks\")\n",
    "print(f\"   Total chunks: {len(chunks)}\")\n",
    "print(f\"   Chunk size: ~1000 characters\")\n",
    "print(f\"   Overlap: 200 characters\")\n",
    "print(f\"\\n   Example chunk:\")\n",
    "print(f\"   {chunks[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d899f3",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "Chunking breaks large documents into smaller, manageable pieces.\n",
    "\n",
    "**Why chunk?**\n",
    "- LLMs have token limits (can't process entire documents at once)\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Each chunk can be embedded and searched independently\n",
    "\n",
    "**Key parameters:**\n",
    "- **chunk_size=1000**: Each chunk is roughly 1000 characters\n",
    "- **chunk_overlap=200**: Chunks share 200 characters to preserve context across boundaries\n",
    "- This prevents important information from being split awkwardly\n",
    "\n",
    "**Example:** If a sentence spans a chunk boundary, the overlap ensures it appears in both chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d8530",
   "metadata": {},
   "source": [
    "## Step 7: Store Chunks in Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Adding chunks to vector store (this may take a moment)...\")\n",
    "\n",
    "document_ids = vector_store.add_documents(chunks)\n",
    "\n",
    "print(f\"‚úÖ All chunks stored in vector database\")\n",
    "print(f\"   Total documents indexed: {len(document_ids)}\")\n",
    "print(f\"   Each chunk has been:\")\n",
    "print(f\"   1. Converted to embedding (vector)\")\n",
    "print(f\"   2. Stored in ChromaDB\")\n",
    "print(f\"   3. Ready for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d078bac",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "This is where the magic happens! Each chunk is:\n",
    "\n",
    "1. **Converted to embedding**: OpenAI's model converts text ‚Üí 3072-dimensional vector\n",
    "2. **Stored in ChromaDB**: Vector + original text saved together\n",
    "3. **Indexed**: Database organizes vectors for fast retrieval\n",
    "\n",
    "**What happens behind the scenes:**\n",
    "```\n",
    "Chunk 1: \"Putin invaded Ukraine...\" ‚Üí [0.234, -0.567, 0.891, ...] (3072 numbers)\n",
    "Chunk 2: \"The economy is strong...\" ‚Üí [0.123, -0.234, 0.456, ...] (3072 numbers)\n",
    "...\n",
    "```\n",
    "\n",
    "Now when you search, ChromaDB can quickly find chunks with similar vectors to your query!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c00c5",
   "metadata": {},
   "source": [
    "## Step 8: Create a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever created\")\n",
    "print(f\"   Will retrieve: Top 3 most similar chunks\")\n",
    "print(f\"   Search method: Similarity search using vector distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254a26e",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "The retriever is responsible for finding relevant documents based on a query.\n",
    "\n",
    "**How it works:**\n",
    "1. Takes a query (e.g., \"Who invaded Ukraine?\")\n",
    "2. Converts query to embedding\n",
    "3. Compares query embedding to all chunk embeddings\n",
    "4. Returns the k=3 most similar chunks\n",
    "\n",
    "**Why k=3?**\n",
    "- Balance between context and token limits\n",
    "- More chunks = more context but longer prompts\n",
    "- Fewer chunks = faster but might miss relevant info\n",
    "- 3 is a good starting point; adjust based on your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8b848",
   "metadata": {},
   "source": [
    "## Step 9: Initialize the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0  # Deterministic outputs for factual answers\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Language Model initialized\")\n",
    "print(f\"   Model: GPT-4o-mini\")\n",
    "print(f\"   Temperature: 0 (factual, consistent answers)\")\n",
    "print(f\"   Purpose: Generate answers based on retrieved context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75db54a",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "The LLM generates the final answer using the retrieved context.\n",
    "\n",
    "**Model choice:**\n",
    "- **GPT-4o-mini**: Faster and cheaper than GPT-4, still high quality\n",
    "- Good balance of performance and cost for RAG applications\n",
    "\n",
    "**Temperature=0:**\n",
    "- Controls randomness in responses\n",
    "- 0 = deterministic, factual (best for RAG)\n",
    "- Higher values = more creative but less reliable\n",
    "\n",
    "The LLM will receive both the retrieved chunks and the user's question, then generate an answer based on that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeda15d",
   "metadata": {},
   "source": [
    "## Step 10: Create the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\n",
    "If the answer is not in the context, say \"I don't have enough information in the provided context to answer that question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(\"‚úÖ Prompt template created\")\n",
    "print(\"   Template has 2 variables:\")\n",
    "print(\"   - {context}: Retrieved chunks will go here\")\n",
    "print(\"   - {question}: User's question will go here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9822b93",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "The prompt template structures how we communicate with the LLM.\n",
    "\n",
    "**Key instructions:**\n",
    "1. **\"Answer based ONLY on the provided context\"** - Prevents hallucination\n",
    "2. **\"Say 'I don't have enough information...' if not in context\"** - Honest responses\n",
    "\n",
    "**Variables:**\n",
    "- `{context}`: Filled with retrieved chunks\n",
    "- `{question}`: Filled with user's query\n",
    "\n",
    "**Why this matters:**\n",
    "Without proper prompting, LLMs might make up answers. This template ensures the LLM only uses the retrieved information, making responses more reliable and trustworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f678b4",
   "metadata": {},
   "source": [
    "## Step 11: Create Document Formatter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Convert list of Document objects to a single formatted string.\"\"\"\n",
    "    return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(\"‚úÖ Document formatter function created\")\n",
    "print(\"   Converts: List of documents ‚Üí Single formatted string\")\n",
    "print(\"   Separates chunks with: ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9fa8d3",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "The retriever returns multiple Document objects, but the prompt needs a single string.\n",
    "\n",
    "**This function:**\n",
    "- Takes a list of retrieved documents\n",
    "- Extracts the text content from each\n",
    "- Joins them with `---` separator for clarity\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Input: [Doc1, Doc2, Doc3]\n",
    "Output: \"Content of Doc1\\n\\n---\\n\\nContent of Doc2\\n\\n---\\n\\nContent of Doc3\"\n",
    "```\n",
    "\n",
    "This formatted string becomes the `{context}` in our prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb37a4",
   "metadata": {},
   "source": [
    "## Step 12: Build the Complete RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e9f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG Chain assembled successfully!\")\n",
    "print(\"\\n   Flow:\")\n",
    "print(\"   1. User Question ‚Üí Retriever ‚Üí Top 3 relevant chunks\")\n",
    "print(\"   2. Chunks ‚Üí format_docs() ‚Üí Single formatted string\")\n",
    "print(\"   3. String + Question ‚Üí Prompt Template ‚Üí Filled prompt\")\n",
    "print(\"   4. Filled Prompt ‚Üí LLM ‚Üí Generated answer\")\n",
    "print(\"   5. Answer ‚Üí StrOutputParser ‚Üí Clean text output\")\n",
    "print(\"\\n   üéâ Ready to answer questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead89bc",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "This is where everything comes together! The RAG chain orchestrates all components.\n",
    "\n",
    "**Chain breakdown:**\n",
    "```\n",
    "{\n",
    "  \"context\": retriever | format_docs,  ‚Üê Retrieve & format docs\n",
    "  \"question\": RunnablePassthrough()    ‚Üê Pass question through unchanged\n",
    "}\n",
    "```\n",
    "- Creates a dictionary with context and question\n",
    "- The `|` operator chains operations together\n",
    "\n",
    "**Then flows through:**\n",
    "1. **prompt**: Fills template with context + question\n",
    "2. **llm**: Generates answer based on filled prompt\n",
    "3. **StrOutputParser()**: Extracts clean text from LLM response\n",
    "\n",
    "**LangChain Expression Language (LCEL):**\n",
    "The `|` syntax is LCEL - a declarative way to build AI pipelines. Clean, composable, and efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87162b26",
   "metadata": {},
   "source": [
    "## Step 13: Test the RAG System - Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e138b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"According to the 2024 State of the Union, who invaded Ukraine?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: Question about document content\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚ùì Question: {question1}\\n\")\n",
    "print(\"üîç Processing... (retrieving relevant chunks and generating answer)\\n\")\n",
    "\n",
    "answer1 = rag_chain.invoke(question1)\n",
    "\n",
    "print(\"‚úÖ Answer:\")\n",
    "print(answer1)\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7908b1",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "This tests a question that IS in the document.\n",
    "\n",
    "**What happens:**\n",
    "1. Question is converted to embedding\n",
    "2. Vector search finds 3 most relevant chunks mentioning Ukraine\n",
    "3. Those chunks are formatted and inserted into prompt\n",
    "4. LLM reads the context and generates an accurate answer\n",
    "5. Answer should correctly identify who invaded Ukraine based on the document\n",
    "\n",
    "**Expected:** The LLM should provide a factual answer grounded in the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba99e971",
   "metadata": {},
   "source": [
    "## Step 14: Test the RAG System - Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0958a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"What is the meaning of life?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 2: Question NOT in document (testing 'I don't know' behavior)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n‚ùì Question: {question2}\\n\")\n",
    "print(\"üîç Processing... (should not find relevant context)\\n\")\n",
    "\n",
    "answer2 = rag_chain.invoke(question2)\n",
    "\n",
    "print(\"‚úÖ Answer:\")\n",
    "print(answer2)\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff422a19",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "This tests a question that is NOT in the document.\n",
    "\n",
    "**What happens:**\n",
    "1. Question is converted to embedding\n",
    "2. Vector search retrieves 3 \"most similar\" chunks (but none are actually relevant)\n",
    "3. LLM reads the irrelevant context\n",
    "4. Because of our prompt instructions, LLM should say \"I don't have enough information...\"\n",
    "\n",
    "**Expected:** The LLM should honestly admit it can't answer based on the provided context.\n",
    "\n",
    "**Why this matters:**\n",
    "This prevents hallucination! Without RAG constraints, an LLM might confidently make up an answer. Our system is honest about its limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1d23f7",
   "metadata": {},
   "source": [
    "## Summary: How RAG Works\n",
    "\n",
    "**üéØ The RAG Pipeline:**\n",
    "\n",
    "```\n",
    "User Question\n",
    "    ‚Üì\n",
    "Embedding Model (converts question to vector)\n",
    "    ‚Üì\n",
    "Vector Store Search (finds similar document chunks)\n",
    "    ‚Üì\n",
    "Retrieved Chunks (top 3 most relevant)\n",
    "    ‚Üì\n",
    "Format as Context\n",
    "    ‚Üì\n",
    "Prompt Template (combines context + question)\n",
    "    ‚Üì\n",
    "Language Model (generates answer from context)\n",
    "    ‚Üì\n",
    "Final Answer\n",
    "```\n",
    "\n",
    "**‚úÖ Key Benefits:**\n",
    "1. **Grounded Answers**: LLM uses actual documents, not just training data\n",
    "2. **Reduced Hallucination**: Clear instructions to only use provided context\n",
    "3. **Up-to-date Info**: Add new documents anytime without retraining\n",
    "4. **Source Attribution**: Can track which chunks were used\n",
    "5. **Cost Effective**: Smaller context windows than full documents\n",
    "\n",
    "**üîß Components We Built:**\n",
    "- ‚úì Embeddings Model (text ‚Üí vectors)\n",
    "- ‚úì Vector Store (ChromaDB)\n",
    "- ‚úì Text Splitter (large doc ‚Üí chunks)\n",
    "- ‚úì Retriever (semantic search)\n",
    "- ‚úì Prompt Template (structured instructions)\n",
    "- ‚úì LLM (answer generation)\n",
    "- ‚úì RAG Chain (orchestrates everything)\n",
    "\n",
    "**üöÄ Next Steps:**\n",
    "- Try different chunk sizes\n",
    "- Experiment with different retrieval methods (MMR, similarity threshold)\n",
    "- Add metadata filtering\n",
    "- Implement conversation history\n",
    "- Use multiple documents\n",
    "- Add source citations to answers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
