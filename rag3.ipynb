{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc29e0a8",
   "metadata": {},
   "source": [
    "# Building a RAG System from Scratch - Step by Step\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances Large Language Models by providing them with relevant context from a knowledge base before generating answers.\n",
    "\n",
    "**Why RAG?**\n",
    "- ‚úÖ Reduces hallucinations by grounding answers in real data\n",
    "- ‚úÖ Enables LLMs to access up-to-date information\n",
    "- ‚úÖ Allows working with private/proprietary documents\n",
    "- ‚úÖ Can cite sources for answers\n",
    "\n",
    "**What we'll build:**\n",
    "A complete RAG system that can answer questions about a document (2024 State of the Union address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac03028",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",              # Core LangChain framework\n",
    "    \"langchain-chroma\",       # Chroma vector store integration\n",
    "    \"langchain-openai\",       # OpenAI models integration\n",
    "    \"langchain-core\",         # Core LangChain utilities\n",
    "    \"python-dotenv\",          # Environment variable management\n",
    "    \"chromadb\"                # Vector database\n",
    "]\n",
    "\n",
    "print(\"Installing RAG dependencies...\\n\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(f\"‚úì {package}\")\n",
    "\n",
    "print(\"\\n‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f0963",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "We install 6 essential packages:\n",
    "- **langchain**: Main framework for building LLM applications\n",
    "- **langchain-chroma**: Allows us to use ChromaDB as our vector database\n",
    "- **langchain-openai**: Provides OpenAI's GPT models and embeddings\n",
    "- **langchain-core**: Core utilities for chains and prompts\n",
    "- **python-dotenv**: Loads API keys from .env file securely\n",
    "- **chromadb**: Lightweight vector database for storing document embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1a899",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load API keys from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All imports successful and environment loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dfd6bb",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "Each import serves a specific purpose in our RAG pipeline:\n",
    "- **Chroma**: Vector database for storing embeddings\n",
    "- **PromptTemplate**: Structures prompts with variables\n",
    "- **RunnablePassthrough**: Passes data through pipeline unchanged\n",
    "- **StrOutputParser**: Extracts text from LLM response\n",
    "- **OpenAIEmbeddings**: Converts text to vector embeddings\n",
    "- **ChatOpenAI**: OpenAI's chat model (GPT)\n",
    "- **CharacterTextSplitter**: Splits large documents into chunks\n",
    "- **load_dotenv()**: Loads your OPENAI_API_KEY from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb8d16",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"‚úÖ Embeddings model initialized\")\n",
    "print(f\"   Model: text-embedding-3-large\")\n",
    "print(f\"   Dimensions: 3072 (vector size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5af8e1",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar concepts have similar vectors.\n",
    "\n",
    "**Why text-embedding-3-large?**\n",
    "- High quality: 3072-dimensional vectors\n",
    "- Captures nuanced meaning\n",
    "- Good for semantic similarity search\n",
    "\n",
    "**Example:** \n",
    "- \"dog\" and \"puppy\" ‚Üí similar vectors\n",
    "- \"dog\" and \"car\" ‚Üí different vectors\n",
    "\n",
    "These embeddings allow us to find relevant documents even when they don't contain exact keyword matches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee9c17",
   "metadata": {},
   "source": [
    "## Step 4: Create Vector Store (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5313cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"state_of_union_rag\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vector store created\")\n",
    "print(f\"   Database: ChromaDB\")\n",
    "print(f\"   Collection: state_of_union_rag\")\n",
    "print(f\"   Ready to store document embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd4b9f",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "ChromaDB is a vector database that stores and retrieves embeddings efficiently.\n",
    "\n",
    "**What it does:**\n",
    "- Stores document embeddings (vectors)\n",
    "- Performs fast similarity searches\n",
    "- Returns the most relevant documents for a query\n",
    "\n",
    "**How it works:**\n",
    "1. Documents ‚Üí Embeddings ‚Üí Stored in ChromaDB\n",
    "2. Query ‚Üí Embedding ‚Üí Search similar vectors\n",
    "3. ChromaDB returns most similar documents\n",
    "\n",
    "**Why ChromaDB?**\n",
    "- Lightweight and easy to use\n",
    "- No separate server needed\n",
    "- Perfect for development and small-to-medium projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeab2e9",
   "metadata": {},
   "source": [
    "## Step 5: Load the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2024_state_of_the_union.txt\", \"r\") as f:\n",
    "    document = f.read()\n",
    "\n",
    "print(\"‚úÖ Document loaded successfully\")\n",
    "print(f\"   File: 2024_state_of_the_union.txt\")\n",
    "print(f\"   Total characters: {len(document):,}\")\n",
    "print(f\"   Total words: ~{len(document.split()):,}\")\n",
    "print(f\"\\n   Preview (first 200 chars):\")\n",
    "print(f\"   {document[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fd6b3",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "We load the document that will serve as our knowledge base.\n",
    "\n",
    "**Why this step?**\n",
    "- RAG needs a source of information to retrieve from\n",
    "- This document contains facts the LLM can reference\n",
    "- In production, this could be PDFs, databases, APIs, etc.\n",
    "\n",
    "**Note:** The document is likely too large to fit in a single LLM prompt (context window), which is why we need RAG and chunking in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ed5f8",
   "metadata": {},
   "source": [
    "## Step 6: Split Document into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,        # Each chunk: ~1000 characters\n",
    "    chunk_overlap=200,      # Overlap: 200 characters between chunks\n",
    "    length_function=len,    # Use character count\n",
    "    separator=\"\\n\"          # Split on newlines when possible\n",
    ")\n",
    "\n",
    "chunks = text_splitter.create_documents([document])\n",
    "\n",
    "print(\"‚úÖ Document split into chunks\")\n",
    "print(f\"   Total chunks: {len(chunks)}\")\n",
    "print(f\"   Chunk size: ~1000 characters\")\n",
    "print(f\"   Overlap: 200 characters\")\n",
    "print(f\"\\n   Example chunk:\")\n",
    "print(f\"   {chunks[0].page_content[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d899f3",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "Chunking breaks large documents into smaller, manageable pieces.\n",
    "\n",
    "**Why chunk?**\n",
    "- LLMs have token limits (can't process entire documents at once)\n",
    "- Smaller chunks = more precise retrieval\n",
    "- Each chunk can be embedded and searched independently\n",
    "\n",
    "**Key parameters:**\n",
    "- **chunk_size=1000**: Each chunk is roughly 1000 characters\n",
    "- **chunk_overlap=200**: Chunks share 200 characters to preserve context across boundaries\n",
    "- This prevents important information from being split awkwardly\n",
    "\n",
    "**Example:** If a sentence spans a chunk boundary, the overlap ensures it appears in both chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d8530",
   "metadata": {},
   "source": [
    "## Step 7: Store Chunks in Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bc319",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚è≥ Adding chunks to vector store (this may take a moment)...\")\n",
    "\n",
    "document_ids = vector_store.add_documents(chunks)\n",
    "\n",
    "print(f\"‚úÖ All chunks stored in vector database\")\n",
    "print(f\"   Total documents indexed: {len(document_ids)}\")\n",
    "print(f\"   Each chunk has been:\")\n",
    "print(f\"   1. Converted to embedding (vector)\")\n",
    "print(f\"   2. Stored in ChromaDB\")\n",
    "print(f\"   3. Ready for similarity search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d078bac",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "This is where the magic happens! Each chunk is:\n",
    "\n",
    "1. **Converted to embedding**: OpenAI's model converts text ‚Üí 3072-dimensional vector\n",
    "2. **Stored in ChromaDB**: Vector + original text saved together\n",
    "3. **Indexed**: Database organizes vectors for fast retrieval\n",
    "\n",
    "**What happens behind the scenes:**\n",
    "```\n",
    "Chunk 1: \"Putin invaded Ukraine...\" ‚Üí [0.234, -0.567, 0.891, ...] (3072 numbers)\n",
    "Chunk 2: \"The economy is strong...\" ‚Üí [0.123, -0.234, 0.456, ...] (3072 numbers)\n",
    "...\n",
    "```\n",
    "\n",
    "Now when you search, ChromaDB can quickly find chunks with similar vectors to your query!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c00c5",
   "metadata": {},
   "source": [
    "## Step 8: Create a Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever created\")\n",
    "print(f\"   Will retrieve: Top 3 most similar chunks\")\n",
    "print(f\"   Search method: Similarity search using vector distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254a26e",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "The retriever is responsible for finding relevant documents based on a query.\n",
    "\n",
    "**How it works:**\n",
    "1. Takes a query (e.g., \"Who invaded Ukraine?\")\n",
    "2. Converts query to embedding\n",
    "3. Compares query embedding to all chunk embeddings\n",
    "4. Returns the k=3 most similar chunks\n",
    "\n",
    "**Why k=3?**\n",
    "- Balance between context and token limits\n",
    "- More chunks = more context but longer prompts\n",
    "- Fewer chunks = faster but might miss relevant info\n",
    "- 3 is a good starting point; adjust based on your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa8b848",
   "metadata": {},
   "source": [
    "## Step 9: Initialize the Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0  # Deterministic outputs for factual answers\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Language Model initialized\")\n",
    "print(f\"   Model: GPT-4o-mini\")\n",
    "print(f\"   Temperature: 0 (factual, consistent answers)\")\n",
    "print(f\"   Purpose: Generate answers based on retrieved context\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75db54a",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "The LLM generates the final answer using the retrieved context.\n",
    "\n",
    "**Model choice:**\n",
    "- **GPT-4o-mini**: Faster and cheaper than GPT-4, still high quality\n",
    "- Good balance of performance and cost for RAG applications\n",
    "\n",
    "**Temperature=0:**\n",
    "- Controls randomness in responses\n",
    "- 0 = deterministic, factual (best for RAG)\n",
    "- Higher values = more creative but less reliable\n",
    "\n",
    "The LLM will receive both the retrieved chunks and the user's question, then generate an answer based on that context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeda15d",
   "metadata": {},
   "source": [
    "## Step 10: Create the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful AI assistant. Answer the question based ONLY on the provided context.\n",
    "If the answer is not in the context, say \"I don't have enough information in the provided context to answer that question.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "print(\"‚úÖ Prompt template created\")\n",
    "print(\"   Template has 2 variables:\")\n",
    "print(\"   - {context}: Retrieved chunks will go here\")\n",
    "print(\"   - {question}: User's question will go here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9822b93",
   "metadata": {},
   "source": [
    "**üìù Explanation:**\n",
    "The prompt template structures how we communicate with the LLM.\n",
    "\n",
    "**Key instructions:**\n",
    "1. **\"Answer based ONLY on the provided context\"** - Prevents hallucination\n",
    "2. **\"Say 'I don't have enough information...' if not in context\"** - Honest responses\n",
    "\n",
    "**Variables:**\n",
    "- `{context}`: Filled with retrieved chunks\n",
    "- `{question}`: Filled with user's query\n",
    "\n",
    "**Why this matters:**\n",
    "Without proper prompting, LLMs might make up answers. This template ensures the LLM only uses the retrieved information, making responses more reliable and trustworthy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f678b4",
   "metadata": {},
   "source": [
    "## Step 11: Create Document Formatter Function"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
