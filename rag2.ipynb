{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c3647",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# RAG Implementation - Step by Step\n",
    "\n",
    "This notebook teaches RAG (Retrieval-Augmented Generation) fundamentals through practical implementation.\n",
    "\n",
    "**What is RAG?**\n",
    "RAG combines retrieval of relevant documents with language model generation to produce answers grounded in actual data.\n",
    "\n",
    "**Steps we'll follow:**\n",
    "1. Install and import dependencies\n",
    "2. Setup vector database\n",
    "3. Load and chunk documents\n",
    "4. Create retriever\n",
    "5. Build RAG chain\n",
    "6. Test with queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11046ae8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of packages needed for RAG\n",
    "packages = [\n",
    "    \"langchain\",\n",
    "    \"langchain-chroma\",\n",
    "    \"langchain-openai\",\n",
    "    \"langchain-core\",\n",
    "    \"python-dotenv\",\n",
    "    \"chromadb\"\n",
    "]\n",
    "\n",
    "print(\"Installing packages...\\n\")\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "        print(f\"✓ {package}\")\n",
    "    except:\n",
    "        print(f\"✗ {package} (already installed)\")\n",
    "\n",
    "print(\"\\n✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd3f190",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0951b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51c0cd",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Embeddings Model\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning.\n",
    "We'll use OpenAI's text-embedding-3-large model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3a36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "print(\"✓ Embeddings model initialized\")\n",
    "print(f\"  Model: text-embedding-3-large\")\n",
    "print(f\"  Type: {type(embeddings).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ad84f",
   "metadata": {},
   "source": [
    "## Step 4: Create Vector Store (ChromaDB)\n",
    "\n",
    "ChromaDB is a lightweight vector database. It will store our document embeddings\n",
    "and allow us to search for similar documents semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"rag_collection\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"✓ Vector store created\")\n",
    "print(f\"  Database: ChromaDB\")\n",
    "print(f\"  Collection: rag_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e9a4e",
   "metadata": {},
   "source": [
    "## Step 5: Load Document\n",
    "\n",
    "Read the document that we want to use for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9917e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the document\n",
    "with open(\"2024_state_of_the_union.txt\") as f:\n",
    "    document_text = f.read()\n",
    "\n",
    "print(\"✓ Document loaded\")\n",
    "print(f\"  Total characters: {len(document_text):,}\")\n",
    "print(f\"  First 200 chars: {document_text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce002a1",
   "metadata": {},
   "source": [
    "## Step 6: Split Document into Chunks\n",
    "\n",
    "Large documents need to be split into smaller chunks so the LLM can process them.\n",
    "We use CharacterTextSplitter to create overlapping chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91764c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,          # Each chunk is ~1000 characters\n",
    "    chunk_overlap=200,        # 200 chars overlap between chunks\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the document into chunks\n",
    "chunks = text_splitter.create_documents([document_text])\n",
    "\n",
    "print(\"✓ Document split into chunks\")\n",
    "print(f\"  Number of chunks: {len(chunks)}\")\n",
    "print(f\"  Chunk size: 1000 characters\")\n",
    "print(f\"  Overlap: 200 characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6758b1",
   "metadata": {},
   "source": [
    "## Step 7: Add Chunks to Vector Store\n",
    "\n",
    "Each chunk will be converted to an embedding and stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = vector_store.add_documents(chunks)\n",
    "\n",
    "print(\"✓ Chunks added to vector store\")\n",
    "print(f\"  Total documents stored: {len(document_ids)}\")\n",
    "print(f\"  Status: Ready for retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7e56a",
   "metadata": {},
   "source": [
    "## Step 8: Initialize the Language Model (LLM)\n",
    "\n",
    "We'll use GPT-4o-mini from OpenAI for generating answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee31119",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "print(\"✓ LLM initialized\")\n",
    "print(f\"  Model: gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9e167e",
   "metadata": {},
   "source": [
    "## Step 9: Create a Retriever\n",
    "\n",
    "A retriever searches the vector store for documents similar to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ade60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "print(\"✓ Retriever created\")\n",
    "print(\"  Function: Searches vector store for relevant documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7c14d1",
   "metadata": {},
   "source": [
    "## Step 10: Create a Prompt Template\n",
    "\n",
    "The prompt tells the LLM how to use the retrieved context to answer the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61060209",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Answer the question based only on the provided context. \n",
    "If the answer is not in the context, say \"I don't know based on the provided context.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "print(\"✓ Prompt template created\")\n",
    "print(\"  Template variables: {context}, {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567f7e3",
   "metadata": {},
   "source": [
    "## Step 11: Create a Function to Format Documents\n",
    "\n",
    "This function converts retrieved documents into a single string for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f45645e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Convert list of documents to formatted string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(\"✓ Document formatter created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a750b6",
   "metadata": {},
   "source": [
    "## Step 12: Build the Complete RAG Chain\n",
    "\n",
    "Combine all components into a single chain:\n",
    "1. Retriever fetches relevant documents\n",
    "2. Formatter converts them to text\n",
    "3. Prompt template fills in context and query\n",
    "4. LLM generates the answer\n",
    "5. Output parser extracts the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93326ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✓ RAG Chain built successfully!\")\n",
    "print(\"\\nChain flow:\")\n",
    "print(\"  Query → Retriever → Format → Prompt → LLM → Output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab6a7a",
   "metadata": {},
   "source": [
    "## Step 13: Test RAG - Query 1 (Within Document)\n",
    "\n",
    "Test with a question related to the document content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"Who invaded Ukraine according to the 2024 State of the Union?\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST QUERY 1\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nQuestion: {query1}\\n\")\n",
    "\n",
    "answer1 = rag_chain.invoke(query1)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(answer1)\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
