{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af04a0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# RAG Implementation - First Steps\n",
    "\n",
    "This notebook demonstrates the foundational concepts of Retrieval-Augmented Generation (RAG):\n",
    "1. Setting up embeddings and vector stores\n",
    "2. Splitting documents into chunks\n",
    "3. Storing and retrieving similar documents\n",
    "4. Building a RAG chain with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f79d5",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b49adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required LangChain packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",\n",
    "    \"langchain-chroma\",\n",
    "    \"langchain-openai\",\n",
    "    \"langchain-core\",\n",
    "    \"python-dotenv\",\n",
    "    \"chromadb\"\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\\n\")\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(f\"✓ {package} installed successfully\")\n",
    "\n",
    "print(\"\\n✓ All packages installed successfully!\")\n",
    "print(\"\\nInstalled packages:\")\n",
    "for package in packages:\n",
    "    print(f\"  - {package}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4284fd",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# load .env file\n",
    "load_dotenv('../.env')\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4e6cf",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Embeddings & ChromaDB Vector Store\n",
    "\n",
    "ChromaDB is an embedded vector database that stores document embeddings.\n",
    "We'll use OpenAI's text-embedding-3-large model to create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfebed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Embeddings Model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize ChromaDB as Vector Store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test_collection\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"✓ Embeddings model initialized: text-embedding-3-large\")\n",
    "print(\"✓ ChromaDB vector store created: test_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c879046",
   "metadata": {},
   "source": [
    "## Step 4: Load and Split Documents\n",
    "\n",
    "We'll read a document and split it into chunks.\n",
    "Each chunk will be embedded and stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in State of the Union Address File\n",
    "with open(\"2024_state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "print(f\"✓ Document loaded\")\n",
    "print(f\"  Total length: {len(state_of_the_union)} characters\")\n",
    "\n",
    "# Initialize Text Splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Create Documents (Chunks) From File\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "print(f\"✓ Document split into {len(texts)} chunks\")\n",
    "print(f\"  Chunk size: 1000 characters\")\n",
    "print(f\"  Overlap: 200 characters\")\n",
    "\n",
    "# Save Document Chunks to Vector Store\n",
    "ids = vector_store.add_documents(texts)\n",
    "\n",
    "print(f\"✓ {len(ids)} document chunks added to vector store\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
