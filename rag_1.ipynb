{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af04a0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# RAG Implementation - First Steps\n",
    "\n",
    "This notebook demonstrates the foundational concepts of Retrieval-Augmented Generation (RAG):\n",
    "1. Setting up embeddings and vector stores\n",
    "2. Splitting documents into chunks\n",
    "3. Storing and retrieving similar documents\n",
    "4. Building a RAG chain with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f79d5",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b49adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required LangChain packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"langchain\",\n",
    "    \"langchain-chroma\",\n",
    "    \"langchain-openai\",\n",
    "    \"langchain-core\",\n",
    "    \"python-dotenv\",\n",
    "    \"chromadb\"\n",
    "]\n",
    "\n",
    "print(\"Installing required packages...\\n\")\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"Installing {package}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(f\"✓ {package} installed successfully\")\n",
    "\n",
    "print(\"\\n✓ All packages installed successfully!\")\n",
    "print(\"\\nInstalled packages:\")\n",
    "for package in packages:\n",
    "    print(f\"  - {package}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4284fd",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# load .env file\n",
    "load_dotenv('../.env')\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4e6cf",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Embeddings & ChromaDB Vector Store\n",
    "\n",
    "ChromaDB is an embedded vector database that stores document embeddings.\n",
    "We'll use OpenAI's text-embedding-3-large model to create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfebed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Embeddings Model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize ChromaDB as Vector Store\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"test_collection\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"✓ Embeddings model initialized: text-embedding-3-large\")\n",
    "print(\"✓ ChromaDB vector store created: test_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c879046",
   "metadata": {},
   "source": [
    "## Step 4: Load and Split Documents\n",
    "\n",
    "We'll read a document and split it into chunks.\n",
    "Each chunk will be embedded and stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in State of the Union Address File\n",
    "with open(\"2024_state_of_the_union.txt\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "print(f\"✓ Document loaded\")\n",
    "print(f\"  Total length: {len(state_of_the_union)} characters\")\n",
    "\n",
    "# Initialize Text Splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Create Documents (Chunks) From File\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "print(f\"✓ Document split into {len(texts)} chunks\")\n",
    "print(f\"  Chunk size: 1000 characters\")\n",
    "print(f\"  Overlap: 200 characters\")\n",
    "\n",
    "# Save Document Chunks to Vector Store\n",
    "ids = vector_store.add_documents(texts)\n",
    "\n",
    "print(f\"✓ {len(ids)} document chunks added to vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65ddff",
   "metadata": {},
   "source": [
    "## Step 5: Test Semantic Similarity Search\n",
    "\n",
    "Now we can search the vector store for semantically similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95169585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Vector Store\n",
    "print(\"Search Query: 'Who invaded Ukraine?'\\n\")\n",
    "results = vector_store.similarity_search(\n",
    "    'Who invaded Ukraine?',\n",
    "    k=2\n",
    ")\n",
    "\n",
    "print(f\"✓ Retrieved {len(results)} most relevant chunks:\\n\")\n",
    "\n",
    "# Print Resulting Chunks\n",
    "for i, res in enumerate(results, 1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(f\"{res.page_content}\")\n",
    "    print(f\"Metadata: {res.metadata}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ffff44",
   "metadata": {},
   "source": [
    "## Step 6: Build the RAG Pipeline\n",
    "\n",
    "Now we create the complete RAG chain:\n",
    "1. **Retriever**: Gets relevant documents from vector store\n",
    "2. **Formatter**: Converts documents to string\n",
    "3. **Prompt**: Templates the context and question\n",
    "4. **LLM**: Generates answer based on context\n",
    "5. **Output Parser**: Extracts the text response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create Document Parsing Function\n",
    "def format_docs(docs):\n",
    "    \"\"\"Convert list of documents to single formatted string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "print(\"✓ Document formatter created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93f2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Set Chroma as the Retriever\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "print(\"✓ Retriever created from vector store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685a542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "print(\"✓ LLM initialized: gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2145ca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the Prompt Template\n",
    "prompt_template = \"\"\"Use the context provided to answer the user's question below. If you do not know the answer based on the context provided, tell the user that you do not know the answer to their question based on the context provided and that you are sorry.\n",
    "\n",
    "context: {context}\n",
    "\n",
    "question: {query}\n",
    "\n",
    "answer: \"\"\"\n",
    "\n",
    "# Create Prompt Instance from template\n",
    "custom_rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "print(\"✓ Prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create the RAG Chain\n",
    "# This chains together: retriever -> formatter -> prompt -> LLM -> output parser\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"query\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✓ RAG chain created successfully!\")\n",
    "print(\"\\nChain structure:\")\n",
    "print(\"  1. Retriever: Gets relevant docs from vector store\")\n",
    "print(\"  2. Formatter: Converts docs to string\")\n",
    "print(\"  3. Prompt: Combines context and query\")\n",
    "print(\"  4. LLM: Generates answer\")\n",
    "print(\"  5. Parser: Extracts text output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f017e7b",
   "metadata": {},
   "source": [
    "## Step 7: Test the RAG Chain\n",
    "\n",
    "Now test the RAG chain with queries related to the document content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93653743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 1: Question about document content\n",
    "print(\"=\" * 70)\n",
    "print(\"TEST 1: Query Based on Document Content\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nQuery: According to the 2024 state of the union address, Who invaded Ukraine?\\n\")\n",
    "\n",
    "response1 = rag_chain.invoke(\"According to the 2024 state of the union address, Who invaded Ukraine?\")\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 2: Question NOT in document (tests \"I don't know\" behavior)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 2: Query NOT Based on Document Content\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nQuery: What is the purpose of life?\\n\")\n",
    "\n",
    "response2 = rag_chain.invoke(\"What is the purpose of life?\")\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(response2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
